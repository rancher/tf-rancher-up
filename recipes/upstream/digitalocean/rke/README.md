# Upstream | DigitalOcean | RKE

This module is used to establish a Rancher (local) management cluster using DigitalOcean and RKE.

Documentation can be found [here](./docs.md).

## Usage

```bash
git clone https://github.com/rancherlabs/tf-rancher-up.git
cd recipes/upstream/digitalocean/rke
```

- Copy `terraform.tfvars.example` to `terraform.tfvars`
- Edit `terraform.tfvars`
  - Update the required variables:
    -  `region` to suit your region
    -  `prefix` to give the resources an identifiable name (eg, your initials or first name)
- Store your DigitalOcean auth token as an environment variable: 
```bash
export TF_VAR_do_token=dop_v1_xxxxxxxxx
```
- Alternatively, you can uncomment `do_token` in `terraform.tfvars` and input the required auth token there.
- Variable `os_type` defines operating system used by DigitalOcean droplets. it is possible to choose between `ubuntu` or `opensuse` but, by default, the variable is defined as `opensuse`
- Define variable `droplet_image` with the name of the OpenSUSE image uploaded to the DigitalOcean account when `os_type` has been defined as `opensuse`. The validated OpenSUSE image is openSUSE-Leap-15.6-Minimal-VM.x86_64-Cloud that can be downloaded [here](https://download.opensuse.org/distribution/leap/15.6/appliances/openSUSE-Leap-15.6-Minimal-VM.x86_64-Cloud.qcow2). The steps to upload an image to Digitalocean can be found [here](https://docs.digitalocean.com/products/custom-images/how-to/upload/). 
- SSH keys will be automatically created if `create_ssh_key_pair` is set to `true` (default).
- Modify the `ssh_key_pair_name` variable to contain the name of a public ssh key stored in DigitalOcean and the `ssh_key_pair_path` variable to contain the local path to it's private key when `create_ssh_key_pair` is set to `false`.
- If an HA cluster need to be deployed, change the `instance_count` variable to 3 or more.
- There are more optional variables which can be tweaked under `terraform.tfvars`.

**NOTE** you may need to use ` terraform init -upgrade` to upgrade provider versions

Execute the below commands to start deployment.

```bash
terraform init
terraform plan
terraform apply
```

The login details will be displayed in the screen once the deployment is successful. It will have the details as below.

```bash
rancher_hostname = "https://rancher.<xx.xx.xx.xx>.sslip.io"
rancher_password = "initial-admin-password"
```

- Destroy the resources when cluster is no more needed.
```bash
terraform destroy
```

**IMPORTANT**: Please retire the services which are deployed using these terraform modules within 48 hours. Soon there will be automation to retire the service automatically after 48 hours but till that is in place it will be the users responsibility to not keep it running more than 48 hours.

### Advanced

Target a specific resource/module to action the changes only for that resource/module

For example, target only the `rke_cluster` resource to re-run the equivalent of `rke up`

```bash
terraform apply -target module.rke.rke_cluster.this -target module.rke.local_file.kube_config_yaml
```

This also updates the kube_config generated by RKE.

### Notes

A log file for the RKE provisioning is written to `rke.log`

See full argument list for each module in use:
  - [DigitalOcean](../../../../modules/infra/digitalocean)
  - [RKE](../../../../modules/distribution/rke)
  - [Rancher](../../../../modules/rancher)

### Known Issues
- `terraform destroy` will timeout while removing the Rancher helm release from the cluster. You can run `terraform destroy` again after the timeout and the deprovisiong process will complete successfully.  
